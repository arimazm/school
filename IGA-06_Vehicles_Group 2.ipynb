{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGA-05. Initial Steps in Image Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Students**\n",
    "- Dodiya Indraraj\n",
    "- Magomedova Zamira\n",
    "- Makarkina Irina\n",
    "- Versotskaya Nataliya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Problem setup**\n",
    "\n",
    "In this assignment you are asked to apply ANNs in order to classify a given silhouette (image) of a vehicle as one of four types\n",
    "-\ta double decker bus (class 0),\n",
    "-\tOpel Manta 400 (class 1),\n",
    "-\tSaab 9000 (class 2), \n",
    "-\tCheverolet van (class 3),\n",
    "\n",
    "using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles. The dataset is given in two files: **veh_train.csv** and **veh_test.csv**. Variable class is the outcome (nominal) variable.\n",
    "\n",
    "One of the principal questions of this assignment is whether PCA can help (or, at least, not make worse) predicting classes.\n",
    "\n",
    "Data source and detailed description: https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load traditional libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA as skPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load KERAS\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "c_nclass = 4\n",
    "\n",
    "list_feats = [\"V1\", \"V2\", \"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\n",
    "                             \"V17\",\"V18\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for your ANN\n",
    "def myANN(input_dim):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(c_nclass, activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "veh_train = pd.read_csv('veh_train.csv', header=0)\n",
    "veh_test = pd.read_csv('veh_test.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the training and the test samples\n",
    "X_train = veh_train.drop('class', axis=1)\n",
    "y_train = veh_train['class']\n",
    "\n",
    "X_test = veh_test.drop('class', axis=1)\n",
    "y_test = veh_test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142657</td>\n",
       "      <td>0.493358</td>\n",
       "      <td>0.048570</td>\n",
       "      <td>0.266144</td>\n",
       "      <td>1.350583</td>\n",
       "      <td>0.331154</td>\n",
       "      <td>-0.211507</td>\n",
       "      <td>0.142691</td>\n",
       "      <td>-0.233227</td>\n",
       "      <td>0.751569</td>\n",
       "      <td>-0.406215</td>\n",
       "      <td>-0.350463</td>\n",
       "      <td>0.268827</td>\n",
       "      <td>-0.318038</td>\n",
       "      <td>-0.084475</td>\n",
       "      <td>0.389751</td>\n",
       "      <td>-0.334073</td>\n",
       "      <td>0.170051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.241760</td>\n",
       "      <td>0.817326</td>\n",
       "      <td>1.519254</td>\n",
       "      <td>1.204229</td>\n",
       "      <td>0.563227</td>\n",
       "      <td>0.331154</td>\n",
       "      <td>1.140490</td>\n",
       "      <td>-1.135304</td>\n",
       "      <td>0.923806</td>\n",
       "      <td>0.682146</td>\n",
       "      <td>1.099655</td>\n",
       "      <td>1.094856</td>\n",
       "      <td>1.360598</td>\n",
       "      <td>0.096104</td>\n",
       "      <td>1.551923</td>\n",
       "      <td>-0.418356</td>\n",
       "      <td>-0.172632</td>\n",
       "      <td>0.036608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101589</td>\n",
       "      <td>-0.640530</td>\n",
       "      <td>-0.015373</td>\n",
       "      <td>-0.308812</td>\n",
       "      <td>0.169549</td>\n",
       "      <td>0.103675</td>\n",
       "      <td>-0.752306</td>\n",
       "      <td>0.653888</td>\n",
       "      <td>-0.618905</td>\n",
       "      <td>-0.359197</td>\n",
       "      <td>-0.918852</td>\n",
       "      <td>-0.745667</td>\n",
       "      <td>-1.459809</td>\n",
       "      <td>-1.284369</td>\n",
       "      <td>-0.084475</td>\n",
       "      <td>-0.302912</td>\n",
       "      <td>1.603217</td>\n",
       "      <td>1.504479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.078570</td>\n",
       "      <td>-0.154578</td>\n",
       "      <td>-0.782686</td>\n",
       "      <td>1.083186</td>\n",
       "      <td>5.418592</td>\n",
       "      <td>9.885314</td>\n",
       "      <td>-0.602084</td>\n",
       "      <td>0.526089</td>\n",
       "      <td>-0.618905</td>\n",
       "      <td>-0.289774</td>\n",
       "      <td>1.676371</td>\n",
       "      <td>-0.655335</td>\n",
       "      <td>0.390135</td>\n",
       "      <td>7.550654</td>\n",
       "      <td>0.529174</td>\n",
       "      <td>-0.187469</td>\n",
       "      <td>-1.464159</td>\n",
       "      <td>-1.698148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.608128</td>\n",
       "      <td>1.951214</td>\n",
       "      <td>1.519254</td>\n",
       "      <td>0.084579</td>\n",
       "      <td>-1.536390</td>\n",
       "      <td>-0.578765</td>\n",
       "      <td>2.582619</td>\n",
       "      <td>-1.902101</td>\n",
       "      <td>2.852195</td>\n",
       "      <td>1.445798</td>\n",
       "      <td>2.925923</td>\n",
       "      <td>2.912796</td>\n",
       "      <td>2.694984</td>\n",
       "      <td>1.752670</td>\n",
       "      <td>-0.289024</td>\n",
       "      <td>-0.418356</td>\n",
       "      <td>-1.302718</td>\n",
       "      <td>-1.698148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.467957</td>\n",
       "      <td>-0.316562</td>\n",
       "      <td>-1.038457</td>\n",
       "      <td>-0.369334</td>\n",
       "      <td>0.432001</td>\n",
       "      <td>0.103675</td>\n",
       "      <td>-0.962616</td>\n",
       "      <td>0.909487</td>\n",
       "      <td>-1.004582</td>\n",
       "      <td>-0.150928</td>\n",
       "      <td>-0.854772</td>\n",
       "      <td>-0.903749</td>\n",
       "      <td>-0.337712</td>\n",
       "      <td>-0.732180</td>\n",
       "      <td>-0.698124</td>\n",
       "      <td>-1.111019</td>\n",
       "      <td>0.634572</td>\n",
       "      <td>0.837265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  0.142657  0.493358  0.048570  0.266144  1.350583  0.331154 -0.211507   \n",
       "1  1.241760  0.817326  1.519254  1.204229  0.563227  0.331154  1.140490   \n",
       "2 -0.101589 -0.640530 -0.015373 -0.308812  0.169549  0.103675 -0.752306   \n",
       "3 -1.078570 -0.154578 -0.782686  1.083186  5.418592  9.885314 -0.602084   \n",
       "4  1.608128  1.951214  1.519254  0.084579 -1.536390 -0.578765  2.582619   \n",
       "5 -0.467957 -0.316562 -1.038457 -0.369334  0.432001  0.103675 -0.962616   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.142691 -0.233227  0.751569 -0.406215 -0.350463  0.268827 -0.318038   \n",
       "1 -1.135304  0.923806  0.682146  1.099655  1.094856  1.360598  0.096104   \n",
       "2  0.653888 -0.618905 -0.359197 -0.918852 -0.745667 -1.459809 -1.284369   \n",
       "3  0.526089 -0.618905 -0.289774  1.676371 -0.655335  0.390135  7.550654   \n",
       "4 -1.902101  2.852195  1.445798  2.925923  2.912796  2.694984  1.752670   \n",
       "5  0.909487 -1.004582 -0.150928 -0.854772 -0.903749 -0.337712 -0.732180   \n",
       "\n",
       "        V15       V16       V17       V18  \n",
       "0 -0.084475  0.389751 -0.334073  0.170051  \n",
       "1  1.551923 -0.418356 -0.172632  0.036608  \n",
       "2 -0.084475 -0.302912  1.603217  1.504479  \n",
       "3  0.529174 -0.187469 -1.464159 -1.698148  \n",
       "4 -0.289024 -0.418356 -1.302718 -1.698148  \n",
       "5 -0.698124 -1.111019  0.634572  0.837265  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since PCA will be applied, let's scale the features\n",
    "\n",
    "# scale the training data\n",
    "X_train_sc = StandardScaler().fit_transform(X_train)\n",
    "X_train_sc = pd.DataFrame(X_train_sc)\n",
    "X_train_sc.columns = list_feats\n",
    "\n",
    "# scale the test data\n",
    "X_test_sc = StandardScaler().fit_transform(X_test)\n",
    "X_test_sc = pd.DataFrame(X_test_sc)\n",
    "X_test_sc.columns = list_feats\n",
    "\n",
    "# preview the scaled data\n",
    "X_train_sc.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (1 point). Descriptive analysis of the ORIGINAL training sample\n",
    "\n",
    "Read the description of the features. Analyze the structure and features of the dataset (no graphs!). Do the following on the **training sample**:\n",
    "\n",
    "-\tCheck whether the classes are balanced.\n",
    "-\tCalculate summary statistics and correlations of the ORIGINAL features.\n",
    "-\tDo you expect that PCA may be helpful? Give brief comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.279762\n",
       "2    0.255952\n",
       "3    0.232143\n",
       "0    0.232143\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the balance of classes\n",
    "y_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.00000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.00000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>168.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>93.059524</td>\n",
       "      <td>44.488095</td>\n",
       "      <td>81.476190</td>\n",
       "      <td>167.87500</td>\n",
       "      <td>61.636905</td>\n",
       "      <td>8.660714</td>\n",
       "      <td>168.029762</td>\n",
       "      <td>41.136905</td>\n",
       "      <td>20.494048</td>\n",
       "      <td>147.291667</td>\n",
       "      <td>188.410714</td>\n",
       "      <td>435.214286</td>\n",
       "      <td>172.958333</td>\n",
       "      <td>73.101190</td>\n",
       "      <td>6.232143</td>\n",
       "      <td>12.50000</td>\n",
       "      <td>188.380952</td>\n",
       "      <td>195.255952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.389973</td>\n",
       "      <td>6.141045</td>\n",
       "      <td>16.284136</td>\n",
       "      <td>35.12905</td>\n",
       "      <td>8.892859</td>\n",
       "      <td>5.353191</td>\n",
       "      <td>33.073213</td>\n",
       "      <td>7.754408</td>\n",
       "      <td>2.587388</td>\n",
       "      <td>14.937536</td>\n",
       "      <td>32.128733</td>\n",
       "      <td>174.852963</td>\n",
       "      <td>30.687795</td>\n",
       "      <td>8.371351</td>\n",
       "      <td>5.033984</td>\n",
       "      <td>9.94837</td>\n",
       "      <td>6.007789</td>\n",
       "      <td>7.198633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>77.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>104.00000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>182.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>86.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>141.00000</td>\n",
       "      <td>56.750000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>143.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>136.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>307.750000</td>\n",
       "      <td>149.750000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.00000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>190.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>92.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>167.00000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>155.500000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.50000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>196.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>99.000000</td>\n",
       "      <td>49.250000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>192.25000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>200.250000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>160.000000</td>\n",
       "      <td>219.000000</td>\n",
       "      <td>599.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.00000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>119.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>333.00000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>182.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>928.000000</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>40.00000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>210.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1          V2          V3         V4          V5          V6  \\\n",
       "count  168.000000  168.000000  168.000000  168.00000  168.000000  168.000000   \n",
       "mean    93.059524   44.488095   81.476190  167.87500   61.636905    8.660714   \n",
       "std      8.389973    6.141045   16.284136   35.12905    8.892859    5.353191   \n",
       "min     77.000000   33.000000   42.000000  104.00000   47.000000    3.000000   \n",
       "25%     86.000000   40.000000   70.000000  141.00000   56.750000    6.750000   \n",
       "50%     92.000000   43.000000   77.500000  167.00000   61.000000    8.000000   \n",
       "75%     99.000000   49.250000   98.000000  192.25000   65.000000   10.000000   \n",
       "max    119.000000   58.000000  112.000000  333.00000  138.000000   55.000000   \n",
       "\n",
       "               V7          V8          V9         V10         V11         V12  \\\n",
       "count  168.000000  168.000000  168.000000  168.000000  168.000000  168.000000   \n",
       "mean   168.029762   41.136905   20.494048  147.291667  188.410714  435.214286   \n",
       "std     33.073213    7.754408    2.587388   14.937536   32.128733  174.852963   \n",
       "min    114.000000   26.000000   17.000000  118.000000  131.000000  191.000000   \n",
       "25%    143.000000   33.000000   19.000000  136.000000  167.000000  307.750000   \n",
       "50%    155.500000   43.000000   19.000000  144.000000  177.000000  359.000000   \n",
       "75%    200.250000   46.000000   23.000000  160.000000  219.000000  599.000000   \n",
       "max    252.000000   59.000000   28.000000  182.000000  320.000000  928.000000   \n",
       "\n",
       "              V13         V14         V15        V16         V17         V18  \n",
       "count  168.000000  168.000000  168.000000  168.00000  168.000000  168.000000  \n",
       "mean   172.958333   73.101190    6.232143   12.50000  188.380952  195.255952  \n",
       "std     30.687795    8.371351    5.033984    9.94837    6.007789    7.198633  \n",
       "min    118.000000   61.000000    0.000000    0.00000  176.000000  182.000000  \n",
       "25%    149.750000   68.000000    2.000000    5.00000  184.000000  190.750000  \n",
       "50%    172.000000   72.000000    5.000000    9.50000  189.000000  196.500000  \n",
       "75%    189.000000   76.000000   10.000000   20.00000  192.000000  201.000000  \n",
       "max    246.000000  135.000000   22.000000   40.00000  203.000000  210.000000  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary statistics on features\n",
    "X_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.699542</td>\n",
       "      <td>0.788050</td>\n",
       "      <td>0.654878</td>\n",
       "      <td>0.080548</td>\n",
       "      <td>0.135643</td>\n",
       "      <td>0.815838</td>\n",
       "      <td>-0.799949</td>\n",
       "      <td>0.817338</td>\n",
       "      <td>0.679767</td>\n",
       "      <td>0.732909</td>\n",
       "      <td>0.820336</td>\n",
       "      <td>0.584789</td>\n",
       "      <td>-0.266598</td>\n",
       "      <td>0.236441</td>\n",
       "      <td>0.181148</td>\n",
       "      <td>0.338834</td>\n",
       "      <td>0.419925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>0.699542</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.818846</td>\n",
       "      <td>0.599700</td>\n",
       "      <td>0.151837</td>\n",
       "      <td>0.234394</td>\n",
       "      <td>0.859786</td>\n",
       "      <td>-0.824165</td>\n",
       "      <td>0.859801</td>\n",
       "      <td>0.972376</td>\n",
       "      <td>0.770759</td>\n",
       "      <td>0.854380</td>\n",
       "      <td>0.933732</td>\n",
       "      <td>-0.015410</td>\n",
       "      <td>0.136358</td>\n",
       "      <td>0.075177</td>\n",
       "      <td>-0.023086</td>\n",
       "      <td>0.153200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>0.788050</td>\n",
       "      <td>0.818846</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.740456</td>\n",
       "      <td>0.157877</td>\n",
       "      <td>0.238783</td>\n",
       "      <td>0.915696</td>\n",
       "      <td>-0.920770</td>\n",
       "      <td>0.899692</td>\n",
       "      <td>0.793874</td>\n",
       "      <td>0.841594</td>\n",
       "      <td>0.902781</td>\n",
       "      <td>0.712613</td>\n",
       "      <td>-0.273401</td>\n",
       "      <td>0.098208</td>\n",
       "      <td>0.307754</td>\n",
       "      <td>0.221113</td>\n",
       "      <td>0.430547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>0.654878</td>\n",
       "      <td>0.599700</td>\n",
       "      <td>0.740456</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.707381</td>\n",
       "      <td>0.516383</td>\n",
       "      <td>0.707801</td>\n",
       "      <td>-0.759417</td>\n",
       "      <td>0.672070</td>\n",
       "      <td>0.552712</td>\n",
       "      <td>0.838437</td>\n",
       "      <td>0.703112</td>\n",
       "      <td>0.515483</td>\n",
       "      <td>-0.086557</td>\n",
       "      <td>0.065924</td>\n",
       "      <td>0.159254</td>\n",
       "      <td>0.436090</td>\n",
       "      <td>0.520999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>0.080548</td>\n",
       "      <td>0.151837</td>\n",
       "      <td>0.157877</td>\n",
       "      <td>0.707381</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.696259</td>\n",
       "      <td>0.089659</td>\n",
       "      <td>-0.160701</td>\n",
       "      <td>0.052084</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.403399</td>\n",
       "      <td>0.084352</td>\n",
       "      <td>0.136533</td>\n",
       "      <td>0.293844</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>-0.071644</td>\n",
       "      <td>0.278881</td>\n",
       "      <td>0.288531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>0.135643</td>\n",
       "      <td>0.234394</td>\n",
       "      <td>0.238783</td>\n",
       "      <td>0.516383</td>\n",
       "      <td>0.696259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.147452</td>\n",
       "      <td>-0.158561</td>\n",
       "      <td>0.131928</td>\n",
       "      <td>0.275023</td>\n",
       "      <td>0.426961</td>\n",
       "      <td>0.132797</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.379453</td>\n",
       "      <td>0.070047</td>\n",
       "      <td>0.021420</td>\n",
       "      <td>0.050590</td>\n",
       "      <td>0.212975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>0.815838</td>\n",
       "      <td>0.859786</td>\n",
       "      <td>0.915696</td>\n",
       "      <td>0.707801</td>\n",
       "      <td>0.089659</td>\n",
       "      <td>0.147452</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.974535</td>\n",
       "      <td>0.991869</td>\n",
       "      <td>0.823876</td>\n",
       "      <td>0.903663</td>\n",
       "      <td>0.997124</td>\n",
       "      <td>0.788788</td>\n",
       "      <td>-0.126122</td>\n",
       "      <td>0.076710</td>\n",
       "      <td>0.300553</td>\n",
       "      <td>0.085229</td>\n",
       "      <td>0.231232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>-0.799949</td>\n",
       "      <td>-0.824165</td>\n",
       "      <td>-0.920770</td>\n",
       "      <td>-0.759417</td>\n",
       "      <td>-0.160701</td>\n",
       "      <td>-0.158561</td>\n",
       "      <td>-0.974535</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.950079</td>\n",
       "      <td>-0.784625</td>\n",
       "      <td>-0.901221</td>\n",
       "      <td>-0.961996</td>\n",
       "      <td>-0.750600</td>\n",
       "      <td>0.203276</td>\n",
       "      <td>-0.067854</td>\n",
       "      <td>-0.255725</td>\n",
       "      <td>-0.209352</td>\n",
       "      <td>-0.328024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>0.817338</td>\n",
       "      <td>0.859801</td>\n",
       "      <td>0.899692</td>\n",
       "      <td>0.672070</td>\n",
       "      <td>0.052084</td>\n",
       "      <td>0.131928</td>\n",
       "      <td>0.991869</td>\n",
       "      <td>-0.950079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.828083</td>\n",
       "      <td>0.881959</td>\n",
       "      <td>0.992103</td>\n",
       "      <td>0.789099</td>\n",
       "      <td>-0.114010</td>\n",
       "      <td>0.097801</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>0.051381</td>\n",
       "      <td>0.202784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>0.679767</td>\n",
       "      <td>0.972376</td>\n",
       "      <td>0.793874</td>\n",
       "      <td>0.552712</td>\n",
       "      <td>0.129094</td>\n",
       "      <td>0.275023</td>\n",
       "      <td>0.823876</td>\n",
       "      <td>-0.784625</td>\n",
       "      <td>0.828083</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.726810</td>\n",
       "      <td>0.814325</td>\n",
       "      <td>0.876480</td>\n",
       "      <td>-0.018817</td>\n",
       "      <td>0.114243</td>\n",
       "      <td>0.089959</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.182122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>0.732909</td>\n",
       "      <td>0.770759</td>\n",
       "      <td>0.841594</td>\n",
       "      <td>0.838437</td>\n",
       "      <td>0.403399</td>\n",
       "      <td>0.426961</td>\n",
       "      <td>0.903663</td>\n",
       "      <td>-0.901221</td>\n",
       "      <td>0.881959</td>\n",
       "      <td>0.726810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.900885</td>\n",
       "      <td>0.731163</td>\n",
       "      <td>0.123274</td>\n",
       "      <td>0.051092</td>\n",
       "      <td>0.269053</td>\n",
       "      <td>0.114991</td>\n",
       "      <td>0.216712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>0.820336</td>\n",
       "      <td>0.854380</td>\n",
       "      <td>0.902781</td>\n",
       "      <td>0.703112</td>\n",
       "      <td>0.084352</td>\n",
       "      <td>0.132797</td>\n",
       "      <td>0.997124</td>\n",
       "      <td>-0.961996</td>\n",
       "      <td>0.992103</td>\n",
       "      <td>0.814325</td>\n",
       "      <td>0.900885</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.785933</td>\n",
       "      <td>-0.121096</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.295456</td>\n",
       "      <td>0.087866</td>\n",
       "      <td>0.222531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>0.584789</td>\n",
       "      <td>0.933732</td>\n",
       "      <td>0.712613</td>\n",
       "      <td>0.515483</td>\n",
       "      <td>0.136533</td>\n",
       "      <td>0.177975</td>\n",
       "      <td>0.788788</td>\n",
       "      <td>-0.750600</td>\n",
       "      <td>0.789099</td>\n",
       "      <td>0.876480</td>\n",
       "      <td>0.731163</td>\n",
       "      <td>0.785933</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.112785</td>\n",
       "      <td>0.160731</td>\n",
       "      <td>0.033354</td>\n",
       "      <td>-0.150714</td>\n",
       "      <td>-0.021582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>-0.266598</td>\n",
       "      <td>-0.015410</td>\n",
       "      <td>-0.273401</td>\n",
       "      <td>-0.086557</td>\n",
       "      <td>0.293844</td>\n",
       "      <td>0.379453</td>\n",
       "      <td>-0.126122</td>\n",
       "      <td>0.203276</td>\n",
       "      <td>-0.114010</td>\n",
       "      <td>-0.018817</td>\n",
       "      <td>0.123274</td>\n",
       "      <td>-0.121096</td>\n",
       "      <td>0.112785</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.017754</td>\n",
       "      <td>-0.061511</td>\n",
       "      <td>-0.660851</td>\n",
       "      <td>-0.701758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>0.236441</td>\n",
       "      <td>0.136358</td>\n",
       "      <td>0.098208</td>\n",
       "      <td>0.065924</td>\n",
       "      <td>0.008716</td>\n",
       "      <td>0.070047</td>\n",
       "      <td>0.076710</td>\n",
       "      <td>-0.067854</td>\n",
       "      <td>0.097801</td>\n",
       "      <td>0.114243</td>\n",
       "      <td>0.051092</td>\n",
       "      <td>0.073300</td>\n",
       "      <td>0.160731</td>\n",
       "      <td>-0.017754</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.042746</td>\n",
       "      <td>0.104174</td>\n",
       "      <td>0.084111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>0.181148</td>\n",
       "      <td>0.075177</td>\n",
       "      <td>0.307754</td>\n",
       "      <td>0.159254</td>\n",
       "      <td>-0.071644</td>\n",
       "      <td>0.021420</td>\n",
       "      <td>0.300553</td>\n",
       "      <td>-0.255725</td>\n",
       "      <td>0.304167</td>\n",
       "      <td>0.089959</td>\n",
       "      <td>0.269053</td>\n",
       "      <td>0.295456</td>\n",
       "      <td>0.033354</td>\n",
       "      <td>-0.061511</td>\n",
       "      <td>0.042746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.000902</td>\n",
       "      <td>0.133407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>0.338834</td>\n",
       "      <td>-0.023086</td>\n",
       "      <td>0.221113</td>\n",
       "      <td>0.436090</td>\n",
       "      <td>0.278881</td>\n",
       "      <td>0.050590</td>\n",
       "      <td>0.085229</td>\n",
       "      <td>-0.209352</td>\n",
       "      <td>0.051381</td>\n",
       "      <td>-0.026134</td>\n",
       "      <td>0.114991</td>\n",
       "      <td>0.087866</td>\n",
       "      <td>-0.150714</td>\n",
       "      <td>-0.660851</td>\n",
       "      <td>0.104174</td>\n",
       "      <td>-0.000902</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.884280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>0.419925</td>\n",
       "      <td>0.153200</td>\n",
       "      <td>0.430547</td>\n",
       "      <td>0.520999</td>\n",
       "      <td>0.288531</td>\n",
       "      <td>0.212975</td>\n",
       "      <td>0.231232</td>\n",
       "      <td>-0.328024</td>\n",
       "      <td>0.202784</td>\n",
       "      <td>0.182122</td>\n",
       "      <td>0.216712</td>\n",
       "      <td>0.222531</td>\n",
       "      <td>-0.021582</td>\n",
       "      <td>-0.701758</td>\n",
       "      <td>0.084111</td>\n",
       "      <td>0.133407</td>\n",
       "      <td>0.884280</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           V1        V2        V3        V4        V5        V6        V7  \\\n",
       "V1   1.000000  0.699542  0.788050  0.654878  0.080548  0.135643  0.815838   \n",
       "V2   0.699542  1.000000  0.818846  0.599700  0.151837  0.234394  0.859786   \n",
       "V3   0.788050  0.818846  1.000000  0.740456  0.157877  0.238783  0.915696   \n",
       "V4   0.654878  0.599700  0.740456  1.000000  0.707381  0.516383  0.707801   \n",
       "V5   0.080548  0.151837  0.157877  0.707381  1.000000  0.696259  0.089659   \n",
       "V6   0.135643  0.234394  0.238783  0.516383  0.696259  1.000000  0.147452   \n",
       "V7   0.815838  0.859786  0.915696  0.707801  0.089659  0.147452  1.000000   \n",
       "V8  -0.799949 -0.824165 -0.920770 -0.759417 -0.160701 -0.158561 -0.974535   \n",
       "V9   0.817338  0.859801  0.899692  0.672070  0.052084  0.131928  0.991869   \n",
       "V10  0.679767  0.972376  0.793874  0.552712  0.129094  0.275023  0.823876   \n",
       "V11  0.732909  0.770759  0.841594  0.838437  0.403399  0.426961  0.903663   \n",
       "V12  0.820336  0.854380  0.902781  0.703112  0.084352  0.132797  0.997124   \n",
       "V13  0.584789  0.933732  0.712613  0.515483  0.136533  0.177975  0.788788   \n",
       "V14 -0.266598 -0.015410 -0.273401 -0.086557  0.293844  0.379453 -0.126122   \n",
       "V15  0.236441  0.136358  0.098208  0.065924  0.008716  0.070047  0.076710   \n",
       "V16  0.181148  0.075177  0.307754  0.159254 -0.071644  0.021420  0.300553   \n",
       "V17  0.338834 -0.023086  0.221113  0.436090  0.278881  0.050590  0.085229   \n",
       "V18  0.419925  0.153200  0.430547  0.520999  0.288531  0.212975  0.231232   \n",
       "\n",
       "           V8        V9       V10       V11       V12       V13       V14  \\\n",
       "V1  -0.799949  0.817338  0.679767  0.732909  0.820336  0.584789 -0.266598   \n",
       "V2  -0.824165  0.859801  0.972376  0.770759  0.854380  0.933732 -0.015410   \n",
       "V3  -0.920770  0.899692  0.793874  0.841594  0.902781  0.712613 -0.273401   \n",
       "V4  -0.759417  0.672070  0.552712  0.838437  0.703112  0.515483 -0.086557   \n",
       "V5  -0.160701  0.052084  0.129094  0.403399  0.084352  0.136533  0.293844   \n",
       "V6  -0.158561  0.131928  0.275023  0.426961  0.132797  0.177975  0.379453   \n",
       "V7  -0.974535  0.991869  0.823876  0.903663  0.997124  0.788788 -0.126122   \n",
       "V8   1.000000 -0.950079 -0.784625 -0.901221 -0.961996 -0.750600  0.203276   \n",
       "V9  -0.950079  1.000000  0.828083  0.881959  0.992103  0.789099 -0.114010   \n",
       "V10 -0.784625  0.828083  1.000000  0.726810  0.814325  0.876480 -0.018817   \n",
       "V11 -0.901221  0.881959  0.726810  1.000000  0.900885  0.731163  0.123274   \n",
       "V12 -0.961996  0.992103  0.814325  0.900885  1.000000  0.785933 -0.121096   \n",
       "V13 -0.750600  0.789099  0.876480  0.731163  0.785933  1.000000  0.112785   \n",
       "V14  0.203276 -0.114010 -0.018817  0.123274 -0.121096  0.112785  1.000000   \n",
       "V15 -0.067854  0.097801  0.114243  0.051092  0.073300  0.160731 -0.017754   \n",
       "V16 -0.255725  0.304167  0.089959  0.269053  0.295456  0.033354 -0.061511   \n",
       "V17 -0.209352  0.051381 -0.026134  0.114991  0.087866 -0.150714 -0.660851   \n",
       "V18 -0.328024  0.202784  0.182122  0.216712  0.222531 -0.021582 -0.701758   \n",
       "\n",
       "          V15       V16       V17       V18  \n",
       "V1   0.236441  0.181148  0.338834  0.419925  \n",
       "V2   0.136358  0.075177 -0.023086  0.153200  \n",
       "V3   0.098208  0.307754  0.221113  0.430547  \n",
       "V4   0.065924  0.159254  0.436090  0.520999  \n",
       "V5   0.008716 -0.071644  0.278881  0.288531  \n",
       "V6   0.070047  0.021420  0.050590  0.212975  \n",
       "V7   0.076710  0.300553  0.085229  0.231232  \n",
       "V8  -0.067854 -0.255725 -0.209352 -0.328024  \n",
       "V9   0.097801  0.304167  0.051381  0.202784  \n",
       "V10  0.114243  0.089959 -0.026134  0.182122  \n",
       "V11  0.051092  0.269053  0.114991  0.216712  \n",
       "V12  0.073300  0.295456  0.087866  0.222531  \n",
       "V13  0.160731  0.033354 -0.150714 -0.021582  \n",
       "V14 -0.017754 -0.061511 -0.660851 -0.701758  \n",
       "V15  1.000000  0.042746  0.104174  0.084111  \n",
       "V16  0.042746  1.000000 -0.000902  0.133407  \n",
       "V17  0.104174 -0.000902  1.000000  0.884280  \n",
       "V18  0.084111  0.133407  0.884280  1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correlation matrix\n",
    "X_test.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give your comments here**\n",
    "\n",
    "Based on descriptive statisctic we see that there are no missing values in the dataset\n",
    "\n",
    "We see high correlation, for example, between V2 (compactness) & V10 (MAX.LENGTH RECTANGULARITY); V2(compactness)&V13(SCALED RADIUS OF GYRATION) etc.\n",
    "\n",
    "V8 (ELONGATEDNESS) has negative correlation with all the features, the strongest is with V7 (SCATTER RATIO), and also high with V3 (DISTANCE CIRCULARITY), V11 (SCALED VARIANCE along major x) & V12 (SCALED VARIANCE along minor x)\n",
    "\n",
    "For PCA variance is important, we see in initial data that features V15, V16, V6 have one-two digit values, while V12 3digit values that are quite high, however stdev is somewhat similar.\n",
    " \n",
    "Classes are somewhat balanced.\n",
    "\n",
    "We expect PCA to be somewhat helpful as there are some some features that are highly correlated with each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (1 point). ANN on the SCALED full dataset\n",
    "\n",
    "Estimate and validate a simple 1-layer ANN on the SCALED full data. Do the following:\n",
    "\n",
    "- Transform the outcome variable on both sample appropriately.\n",
    "- Run the model (use 100 epochs).\n",
    "- Validate its performance.\n",
    "- Give brief comments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the outcome variable\n",
    "Y_01_train = np_utils.to_categorical(y_train, c_nclass)\n",
    "Y_01_test = np_utils.to_categorical(y_test, c_nclass) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setup the Model1 with the myANN function\n",
    "model1 = myANN(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 678 samples, validate on 168 samples\n",
      "Epoch 1/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 1.2541 - accuracy: 0.4602 - val_loss: 1.1833 - val_accuracy: 0.4821\n",
      "Epoch 2/100\n",
      "678/678 [==============================] - 0s 117us/step - loss: 1.0451 - accuracy: 0.5590 - val_loss: 1.0675 - val_accuracy: 0.5595\n",
      "Epoch 3/100\n",
      "678/678 [==============================] - 0s 117us/step - loss: 0.9257 - accuracy: 0.6342 - val_loss: 0.9796 - val_accuracy: 0.6369\n",
      "Epoch 4/100\n",
      "678/678 [==============================] - 0s 146us/step - loss: 0.8420 - accuracy: 0.6770 - val_loss: 0.9275 - val_accuracy: 0.6190\n",
      "Epoch 5/100\n",
      "678/678 [==============================] - 0s 124us/step - loss: 0.7748 - accuracy: 0.6932 - val_loss: 0.8595 - val_accuracy: 0.6845\n",
      "Epoch 6/100\n",
      "678/678 [==============================] - 0s 116us/step - loss: 0.7157 - accuracy: 0.7448 - val_loss: 0.8180 - val_accuracy: 0.6905\n",
      "Epoch 7/100\n",
      "678/678 [==============================] - 0s 112us/step - loss: 0.6667 - accuracy: 0.7419 - val_loss: 0.7922 - val_accuracy: 0.6905\n",
      "Epoch 8/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.6275 - accuracy: 0.7640 - val_loss: 0.7444 - val_accuracy: 0.7024\n",
      "Epoch 9/100\n",
      "678/678 [==============================] - 0s 190us/step - loss: 0.5959 - accuracy: 0.7743 - val_loss: 0.7181 - val_accuracy: 0.7143\n",
      "Epoch 10/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.5701 - accuracy: 0.7699 - val_loss: 0.7056 - val_accuracy: 0.7143\n",
      "Epoch 11/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.5434 - accuracy: 0.7773 - val_loss: 0.6785 - val_accuracy: 0.7143\n",
      "Epoch 12/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.5283 - accuracy: 0.7817 - val_loss: 0.6759 - val_accuracy: 0.7024\n",
      "Epoch 13/100\n",
      "678/678 [==============================] - 0s 113us/step - loss: 0.5088 - accuracy: 0.7861 - val_loss: 0.6550 - val_accuracy: 0.7024\n",
      "Epoch 14/100\n",
      "678/678 [==============================] - 0s 102us/step - loss: 0.4902 - accuracy: 0.7920 - val_loss: 0.6527 - val_accuracy: 0.7262\n",
      "Epoch 15/100\n",
      "678/678 [==============================] - 0s 102us/step - loss: 0.4774 - accuracy: 0.8024 - val_loss: 0.6400 - val_accuracy: 0.7321\n",
      "Epoch 16/100\n",
      "678/678 [==============================] - 0s 109us/step - loss: 0.4656 - accuracy: 0.8142 - val_loss: 0.6164 - val_accuracy: 0.7321\n",
      "Epoch 17/100\n",
      "678/678 [==============================] - 0s 104us/step - loss: 0.4510 - accuracy: 0.8142 - val_loss: 0.6057 - val_accuracy: 0.7381\n",
      "Epoch 18/100\n",
      "678/678 [==============================] - 0s 95us/step - loss: 0.4417 - accuracy: 0.8230 - val_loss: 0.6051 - val_accuracy: 0.7381\n",
      "Epoch 19/100\n",
      "678/678 [==============================] - 0s 93us/step - loss: 0.4339 - accuracy: 0.8245 - val_loss: 0.6020 - val_accuracy: 0.7321\n",
      "Epoch 20/100\n",
      "678/678 [==============================] - 0s 94us/step - loss: 0.4255 - accuracy: 0.8127 - val_loss: 0.5975 - val_accuracy: 0.7321\n",
      "Epoch 21/100\n",
      "678/678 [==============================] - 0s 98us/step - loss: 0.4134 - accuracy: 0.8289 - val_loss: 0.5828 - val_accuracy: 0.7500\n",
      "Epoch 22/100\n",
      "678/678 [==============================] - 0s 115us/step - loss: 0.4057 - accuracy: 0.8260 - val_loss: 0.5785 - val_accuracy: 0.7440\n",
      "Epoch 23/100\n",
      "678/678 [==============================] - 0s 151us/step - loss: 0.3978 - accuracy: 0.8304 - val_loss: 0.5771 - val_accuracy: 0.7381\n",
      "Epoch 24/100\n",
      "678/678 [==============================] - 0s 174us/step - loss: 0.3913 - accuracy: 0.8392 - val_loss: 0.5746 - val_accuracy: 0.7500\n",
      "Epoch 25/100\n",
      "678/678 [==============================] - 0s 135us/step - loss: 0.3866 - accuracy: 0.8407 - val_loss: 0.5730 - val_accuracy: 0.7321\n",
      "Epoch 26/100\n",
      "678/678 [==============================] - 0s 114us/step - loss: 0.3866 - accuracy: 0.8289 - val_loss: 0.5571 - val_accuracy: 0.7619\n",
      "Epoch 27/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.3760 - accuracy: 0.8481 - val_loss: 0.5811 - val_accuracy: 0.7083\n",
      "Epoch 28/100\n",
      "678/678 [==============================] - 0s 112us/step - loss: 0.3716 - accuracy: 0.8481 - val_loss: 0.5604 - val_accuracy: 0.7440\n",
      "Epoch 29/100\n",
      "678/678 [==============================] - 0s 148us/step - loss: 0.3620 - accuracy: 0.8584 - val_loss: 0.5489 - val_accuracy: 0.7381\n",
      "Epoch 30/100\n",
      "678/678 [==============================] - 0s 168us/step - loss: 0.3560 - accuracy: 0.8540 - val_loss: 0.5434 - val_accuracy: 0.7560\n",
      "Epoch 31/100\n",
      "678/678 [==============================] - 0s 159us/step - loss: 0.3538 - accuracy: 0.8643 - val_loss: 0.5551 - val_accuracy: 0.7143\n",
      "Epoch 32/100\n",
      "678/678 [==============================] - 0s 162us/step - loss: 0.3485 - accuracy: 0.8525 - val_loss: 0.5391 - val_accuracy: 0.7738\n",
      "Epoch 33/100\n",
      "678/678 [==============================] - 0s 119us/step - loss: 0.3421 - accuracy: 0.8687 - val_loss: 0.5365 - val_accuracy: 0.7381\n",
      "Epoch 34/100\n",
      "678/678 [==============================] - 0s 106us/step - loss: 0.3375 - accuracy: 0.8864 - val_loss: 0.5382 - val_accuracy: 0.7262\n",
      "Epoch 35/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.3365 - accuracy: 0.8628 - val_loss: 0.5343 - val_accuracy: 0.7738\n",
      "Epoch 36/100\n",
      "678/678 [==============================] - 0s 106us/step - loss: 0.3338 - accuracy: 0.8732 - val_loss: 0.5273 - val_accuracy: 0.7500\n",
      "Epoch 37/100\n",
      "678/678 [==============================] - 0s 145us/step - loss: 0.3249 - accuracy: 0.8879 - val_loss: 0.5324 - val_accuracy: 0.7679\n",
      "Epoch 38/100\n",
      "678/678 [==============================] - 0s 177us/step - loss: 0.3278 - accuracy: 0.8599 - val_loss: 0.5190 - val_accuracy: 0.7679\n",
      "Epoch 39/100\n",
      "678/678 [==============================] - 0s 152us/step - loss: 0.3190 - accuracy: 0.8732 - val_loss: 0.5308 - val_accuracy: 0.7321\n",
      "Epoch 40/100\n",
      "678/678 [==============================] - 0s 168us/step - loss: 0.3168 - accuracy: 0.8717 - val_loss: 0.5194 - val_accuracy: 0.7738\n",
      "Epoch 41/100\n",
      "678/678 [==============================] - 0s 171us/step - loss: 0.3117 - accuracy: 0.8850 - val_loss: 0.5143 - val_accuracy: 0.7440\n",
      "Epoch 42/100\n",
      "678/678 [==============================] - 0s 169us/step - loss: 0.3059 - accuracy: 0.8894 - val_loss: 0.5136 - val_accuracy: 0.7560\n",
      "Epoch 43/100\n",
      "678/678 [==============================] - 0s 161us/step - loss: 0.3089 - accuracy: 0.8761 - val_loss: 0.5151 - val_accuracy: 0.7619\n",
      "Epoch 44/100\n",
      "678/678 [==============================] - 0s 131us/step - loss: 0.3020 - accuracy: 0.8761 - val_loss: 0.5168 - val_accuracy: 0.7619\n",
      "Epoch 45/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.2966 - accuracy: 0.8894 - val_loss: 0.5116 - val_accuracy: 0.7560\n",
      "Epoch 46/100\n",
      "678/678 [==============================] - 0s 95us/step - loss: 0.2956 - accuracy: 0.8953 - val_loss: 0.5137 - val_accuracy: 0.7798\n",
      "Epoch 47/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.2974 - accuracy: 0.9012 - val_loss: 0.5193 - val_accuracy: 0.7857\n",
      "Epoch 48/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.2886 - accuracy: 0.8909 - val_loss: 0.5028 - val_accuracy: 0.7857\n",
      "Epoch 49/100\n",
      "678/678 [==============================] - 0s 141us/step - loss: 0.2877 - accuracy: 0.8894 - val_loss: 0.5113 - val_accuracy: 0.7679\n",
      "Epoch 50/100\n",
      "678/678 [==============================] - 0s 167us/step - loss: 0.2856 - accuracy: 0.8835 - val_loss: 0.5079 - val_accuracy: 0.7679\n",
      "Epoch 51/100\n",
      "678/678 [==============================] - 0s 156us/step - loss: 0.2835 - accuracy: 0.8982 - val_loss: 0.4955 - val_accuracy: 0.7738\n",
      "Epoch 52/100\n",
      "678/678 [==============================] - 0s 157us/step - loss: 0.2770 - accuracy: 0.9100 - val_loss: 0.5067 - val_accuracy: 0.7738\n",
      "Epoch 53/100\n",
      "678/678 [==============================] - 0s 162us/step - loss: 0.2760 - accuracy: 0.8938 - val_loss: 0.4988 - val_accuracy: 0.7798\n",
      "Epoch 54/100\n",
      "678/678 [==============================] - 0s 118us/step - loss: 0.2723 - accuracy: 0.9071 - val_loss: 0.5056 - val_accuracy: 0.7738\n",
      "Epoch 55/100\n",
      "678/678 [==============================] - 0s 108us/step - loss: 0.2754 - accuracy: 0.9056 - val_loss: 0.5149 - val_accuracy: 0.7560\n",
      "Epoch 56/100\n",
      "678/678 [==============================] - 0s 95us/step - loss: 0.2732 - accuracy: 0.8982 - val_loss: 0.4995 - val_accuracy: 0.7738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.2691 - accuracy: 0.9041 - val_loss: 0.4896 - val_accuracy: 0.7917\n",
      "Epoch 58/100\n",
      "678/678 [==============================] - 0s 112us/step - loss: 0.2643 - accuracy: 0.9071 - val_loss: 0.4866 - val_accuracy: 0.7857\n",
      "Epoch 59/100\n",
      "678/678 [==============================] - 0s 160us/step - loss: 0.2619 - accuracy: 0.9056 - val_loss: 0.4976 - val_accuracy: 0.7738\n",
      "Epoch 60/100\n",
      "678/678 [==============================] - 0s 165us/step - loss: 0.2566 - accuracy: 0.9204 - val_loss: 0.5067 - val_accuracy: 0.7857\n",
      "Epoch 61/100\n",
      "678/678 [==============================] - 0s 165us/step - loss: 0.2592 - accuracy: 0.9012 - val_loss: 0.4829 - val_accuracy: 0.7857\n",
      "Epoch 62/100\n",
      "678/678 [==============================] - 0s 159us/step - loss: 0.2589 - accuracy: 0.9086 - val_loss: 0.4888 - val_accuracy: 0.7857\n",
      "Epoch 63/100\n",
      "678/678 [==============================] - 0s 158us/step - loss: 0.2500 - accuracy: 0.9145 - val_loss: 0.4896 - val_accuracy: 0.7857\n",
      "Epoch 64/100\n",
      "678/678 [==============================] - 0s 111us/step - loss: 0.2478 - accuracy: 0.9159 - val_loss: 0.4864 - val_accuracy: 0.7798\n",
      "Epoch 65/100\n",
      "678/678 [==============================] - 0s 94us/step - loss: 0.2471 - accuracy: 0.9174 - val_loss: 0.4876 - val_accuracy: 0.7798\n",
      "Epoch 66/100\n",
      "678/678 [==============================] - 0s 95us/step - loss: 0.2450 - accuracy: 0.9189 - val_loss: 0.4837 - val_accuracy: 0.7917\n",
      "Epoch 67/100\n",
      "678/678 [==============================] - 0s 111us/step - loss: 0.2569 - accuracy: 0.8850 - val_loss: 0.4920 - val_accuracy: 0.7798\n",
      "Epoch 68/100\n",
      "678/678 [==============================] - 0s 122us/step - loss: 0.2452 - accuracy: 0.9159 - val_loss: 0.4849 - val_accuracy: 0.7857\n",
      "Epoch 69/100\n",
      "678/678 [==============================] - 0s 155us/step - loss: 0.2438 - accuracy: 0.9159 - val_loss: 0.4771 - val_accuracy: 0.7857\n",
      "Epoch 70/100\n",
      "678/678 [==============================] - 0s 165us/step - loss: 0.2365 - accuracy: 0.9263 - val_loss: 0.4921 - val_accuracy: 0.7857\n",
      "Epoch 71/100\n",
      "678/678 [==============================] - 0s 160us/step - loss: 0.2382 - accuracy: 0.9218 - val_loss: 0.4963 - val_accuracy: 0.7738\n",
      "Epoch 72/100\n",
      "678/678 [==============================] - 0s 117us/step - loss: 0.2330 - accuracy: 0.9233 - val_loss: 0.4890 - val_accuracy: 0.7857\n",
      "Epoch 73/100\n",
      "678/678 [==============================] - 0s 95us/step - loss: 0.2349 - accuracy: 0.9277 - val_loss: 0.4823 - val_accuracy: 0.7917\n",
      "Epoch 74/100\n",
      "678/678 [==============================] - 0s 94us/step - loss: 0.2308 - accuracy: 0.9159 - val_loss: 0.4863 - val_accuracy: 0.7798\n",
      "Epoch 75/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.2281 - accuracy: 0.9322 - val_loss: 0.4837 - val_accuracy: 0.7857\n",
      "Epoch 76/100\n",
      "678/678 [==============================] - 0s 98us/step - loss: 0.2252 - accuracy: 0.9248 - val_loss: 0.4872 - val_accuracy: 0.7857\n",
      "Epoch 77/100\n",
      "678/678 [==============================] - 0s 121us/step - loss: 0.2355 - accuracy: 0.9086 - val_loss: 0.4812 - val_accuracy: 0.7857\n",
      "Epoch 78/100\n",
      "678/678 [==============================] - 0s 161us/step - loss: 0.2271 - accuracy: 0.9174 - val_loss: 0.4759 - val_accuracy: 0.7857\n",
      "Epoch 79/100\n",
      "678/678 [==============================] - 0s 158us/step - loss: 0.2246 - accuracy: 0.9248 - val_loss: 0.4805 - val_accuracy: 0.7857\n",
      "Epoch 80/100\n",
      "678/678 [==============================] - 0s 156us/step - loss: 0.2239 - accuracy: 0.9248 - val_loss: 0.4784 - val_accuracy: 0.7798\n",
      "Epoch 81/100\n",
      "678/678 [==============================] - 0s 157us/step - loss: 0.2259 - accuracy: 0.9189 - val_loss: 0.4946 - val_accuracy: 0.7798\n",
      "Epoch 82/100\n",
      "678/678 [==============================] - 0s 154us/step - loss: 0.2190 - accuracy: 0.9218 - val_loss: 0.4832 - val_accuracy: 0.7917\n",
      "Epoch 83/100\n",
      "678/678 [==============================] - 0s 109us/step - loss: 0.2149 - accuracy: 0.9322 - val_loss: 0.4772 - val_accuracy: 0.7738\n",
      "Epoch 84/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.2133 - accuracy: 0.9292 - val_loss: 0.4720 - val_accuracy: 0.7857\n",
      "Epoch 85/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.2132 - accuracy: 0.9322 - val_loss: 0.4817 - val_accuracy: 0.7857\n",
      "Epoch 86/100\n",
      "678/678 [==============================] - 0s 122us/step - loss: 0.2106 - accuracy: 0.9322 - val_loss: 0.4724 - val_accuracy: 0.7857\n",
      "Epoch 87/100\n",
      "678/678 [==============================] - 0s 139us/step - loss: 0.2108 - accuracy: 0.9395 - val_loss: 0.4843 - val_accuracy: 0.7619\n",
      "Epoch 88/100\n",
      "678/678 [==============================] - 0s 202us/step - loss: 0.2071 - accuracy: 0.9351 - val_loss: 0.4760 - val_accuracy: 0.7798\n",
      "Epoch 89/100\n",
      "678/678 [==============================] - 0s 157us/step - loss: 0.2060 - accuracy: 0.9351 - val_loss: 0.4796 - val_accuracy: 0.7738\n",
      "Epoch 90/100\n",
      "678/678 [==============================] - 0s 163us/step - loss: 0.2040 - accuracy: 0.9381 - val_loss: 0.4727 - val_accuracy: 0.7857\n",
      "Epoch 91/100\n",
      "678/678 [==============================] - 0s 167us/step - loss: 0.2044 - accuracy: 0.9307 - val_loss: 0.4696 - val_accuracy: 0.7917\n",
      "Epoch 92/100\n",
      "678/678 [==============================] - 0s 162us/step - loss: 0.2021 - accuracy: 0.9395 - val_loss: 0.4832 - val_accuracy: 0.7798\n",
      "Epoch 93/100\n",
      "678/678 [==============================] - 0s 166us/step - loss: 0.2066 - accuracy: 0.9292 - val_loss: 0.4587 - val_accuracy: 0.7917\n",
      "Epoch 94/100\n",
      "678/678 [==============================] - 0s 129us/step - loss: 0.2013 - accuracy: 0.9395 - val_loss: 0.4795 - val_accuracy: 0.7798\n",
      "Epoch 95/100\n",
      "678/678 [==============================] - 0s 97us/step - loss: 0.1988 - accuracy: 0.9351 - val_loss: 0.4758 - val_accuracy: 0.7857\n",
      "Epoch 96/100\n",
      "678/678 [==============================] - 0s 103us/step - loss: 0.2010 - accuracy: 0.9292 - val_loss: 0.4741 - val_accuracy: 0.7857\n",
      "Epoch 97/100\n",
      "678/678 [==============================] - 0s 106us/step - loss: 0.1999 - accuracy: 0.9322 - val_loss: 0.4663 - val_accuracy: 0.7738\n",
      "Epoch 98/100\n",
      "678/678 [==============================] - 0s 107us/step - loss: 0.1956 - accuracy: 0.9351 - val_loss: 0.4585 - val_accuracy: 0.7798\n",
      "Epoch 99/100\n",
      "678/678 [==============================] - 0s 98us/step - loss: 0.2004 - accuracy: 0.9292 - val_loss: 0.4749 - val_accuracy: 0.7738\n",
      "Epoch 100/100\n",
      "678/678 [==============================] - 0s 103us/step - loss: 0.1923 - accuracy: 0.9410 - val_loss: 0.4742 - val_accuracy: 0.7798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a423d2e10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Model1\n",
    "np.random.seed(12345) # do not change it!\n",
    "model1.fit(X_train_sc, Y_01_train, epochs=100, validation_data = (X_test_sc, Y_01_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        39\n",
      "           1       0.69      0.57      0.63        47\n",
      "           2       0.62      0.65      0.64        43\n",
      "           3       0.88      0.95      0.91        39\n",
      "\n",
      "   micro avg       0.79      0.77      0.78       168\n",
      "   macro avg       0.79      0.79      0.79       168\n",
      "weighted avg       0.78      0.77      0.78       168\n",
      " samples avg       0.77      0.77      0.77       168\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/useruser/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# prediction of classes\n",
    "Y_01_test_pred = model1.predict(X_test_sc)\n",
    "Y_01_test_pred[0]\n",
    "# classification report\n",
    "print(classification_report(y_true=Y_01_test, y_pred=Y_01_test_pred.round(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give your comments here**\n",
    "\n",
    "So the final accuracy is 77,98%, which is quite low, and classification report shows good results, predicting 0 and 3rd classes with high accuracy.\n",
    "\n",
    "We expect that maybe PCA analysis will provide us with a better quality of classification of class 1 and 2, due to high correlation of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (2 points). PCA analysis\n",
    "\n",
    "- Run a PCA analysis on the scaled features.\n",
    "- Calculate variances and cumulative variances.\n",
    "- For further analysis, leave only first 9 principal components. How much variance of the original dataset do they explain?\n",
    "- Give brief comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run a PCA\n",
    "pca = skPCA(n_components=None)\n",
    "pca.fit(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.524, 0.173, 0.101, 0.067, 0.049, 0.03 , 0.02 , 0.012, 0.009,\n",
       "       0.005, 0.003, 0.002, 0.002, 0.001, 0.001, 0.001, 0.   , 0.   ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of variance explained by PCs\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "var_exp.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52367961, 0.69650739, 0.7979649 , 0.8647294 , 0.91364211,\n",
       "       0.94337965, 0.96327801, 0.97549201, 0.98431741, 0.98941662,\n",
       "       0.9926432 , 0.99506694, 0.99702208, 0.99810084, 0.99892997,\n",
       "       0.99963912, 0.99998021, 1.        ])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cumulative variance\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "cum_var_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make first 9 components - for the training and test samples\n",
    "pca = skPCA(n_components=9) #we leave only 9 components\n",
    "pca.fit(X_train_sc)\n",
    "PC_X_train = pca.transform(X_train_sc)\n",
    "PC_X_test = pca.transform(X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give your comments here**\n",
    "\n",
    "So, the first 9 variables explain 98% of the variance, so we expect that leaving only those 9 components will provide better classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 (1 point). ANN on the PCA-transformed data\n",
    "\n",
    "- Re-run the ANN on the chosen 9 PCs.\n",
    "- Estimate its performance on the test data.\n",
    "- Give brief comments on the resulting performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setup the Model2 with the myANN function\n",
    "model2 = myANN(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 678 samples, validate on 168 samples\n",
      "Epoch 1/100\n",
      "678/678 [==============================] - 1s 2ms/step - loss: 1.3390 - accuracy: 0.3274 - val_loss: 1.2514 - val_accuracy: 0.4345\n",
      "Epoch 2/100\n",
      "678/678 [==============================] - 0s 123us/step - loss: 1.1296 - accuracy: 0.5560 - val_loss: 1.1475 - val_accuracy: 0.5595\n",
      "Epoch 3/100\n",
      "678/678 [==============================] - 0s 133us/step - loss: 1.0103 - accuracy: 0.6519 - val_loss: 1.0634 - val_accuracy: 0.6071\n",
      "Epoch 4/100\n",
      "678/678 [==============================] - 0s 133us/step - loss: 0.9190 - accuracy: 0.7021 - val_loss: 0.9844 - val_accuracy: 0.6429\n",
      "Epoch 5/100\n",
      "678/678 [==============================] - 0s 133us/step - loss: 0.8410 - accuracy: 0.7242 - val_loss: 0.9323 - val_accuracy: 0.6369\n",
      "Epoch 6/100\n",
      "678/678 [==============================] - 0s 127us/step - loss: 0.7784 - accuracy: 0.7463 - val_loss: 0.8839 - val_accuracy: 0.6429\n",
      "Epoch 7/100\n",
      "678/678 [==============================] - 0s 134us/step - loss: 0.7233 - accuracy: 0.7566 - val_loss: 0.8488 - val_accuracy: 0.6667\n",
      "Epoch 8/100\n",
      "678/678 [==============================] - 0s 134us/step - loss: 0.6836 - accuracy: 0.7522 - val_loss: 0.8241 - val_accuracy: 0.6845\n",
      "Epoch 9/100\n",
      "678/678 [==============================] - 0s 130us/step - loss: 0.6467 - accuracy: 0.7684 - val_loss: 0.7824 - val_accuracy: 0.6905\n",
      "Epoch 10/100\n",
      "678/678 [==============================] - 0s 128us/step - loss: 0.6184 - accuracy: 0.7655 - val_loss: 0.7735 - val_accuracy: 0.6726\n",
      "Epoch 11/100\n",
      "678/678 [==============================] - 0s 139us/step - loss: 0.5919 - accuracy: 0.7684 - val_loss: 0.7587 - val_accuracy: 0.6726\n",
      "Epoch 12/100\n",
      "678/678 [==============================] - 0s 139us/step - loss: 0.5714 - accuracy: 0.7788 - val_loss: 0.7450 - val_accuracy: 0.6667\n",
      "Epoch 13/100\n",
      "678/678 [==============================] - 0s 131us/step - loss: 0.5526 - accuracy: 0.7788 - val_loss: 0.7365 - val_accuracy: 0.6845\n",
      "Epoch 14/100\n",
      "678/678 [==============================] - 0s 155us/step - loss: 0.5370 - accuracy: 0.7743 - val_loss: 0.7339 - val_accuracy: 0.6726\n",
      "Epoch 15/100\n",
      "678/678 [==============================] - 0s 160us/step - loss: 0.5248 - accuracy: 0.7802 - val_loss: 0.6987 - val_accuracy: 0.6964\n",
      "Epoch 16/100\n",
      "678/678 [==============================] - 0s 137us/step - loss: 0.5133 - accuracy: 0.7773 - val_loss: 0.6976 - val_accuracy: 0.6786\n",
      "Epoch 17/100\n",
      "678/678 [==============================] - 0s 129us/step - loss: 0.5001 - accuracy: 0.7773 - val_loss: 0.6956 - val_accuracy: 0.6845\n",
      "Epoch 18/100\n",
      "678/678 [==============================] - 0s 166us/step - loss: 0.4897 - accuracy: 0.7832 - val_loss: 0.6889 - val_accuracy: 0.6667\n",
      "Epoch 19/100\n",
      "678/678 [==============================] - 0s 142us/step - loss: 0.4825 - accuracy: 0.7920 - val_loss: 0.6810 - val_accuracy: 0.7024\n",
      "Epoch 20/100\n",
      "678/678 [==============================] - 0s 115us/step - loss: 0.4750 - accuracy: 0.8038 - val_loss: 0.6874 - val_accuracy: 0.6786\n",
      "Epoch 21/100\n",
      "678/678 [==============================] - 0s 92us/step - loss: 0.4661 - accuracy: 0.8083 - val_loss: 0.6810 - val_accuracy: 0.6964\n",
      "Epoch 22/100\n",
      "678/678 [==============================] - 0s 93us/step - loss: 0.4583 - accuracy: 0.8024 - val_loss: 0.6815 - val_accuracy: 0.6845\n",
      "Epoch 23/100\n",
      "678/678 [==============================] - 0s 93us/step - loss: 0.4520 - accuracy: 0.7979 - val_loss: 0.6769 - val_accuracy: 0.6667\n",
      "Epoch 24/100\n",
      "678/678 [==============================] - 0s 97us/step - loss: 0.4451 - accuracy: 0.7950 - val_loss: 0.6777 - val_accuracy: 0.6786\n",
      "Epoch 25/100\n",
      "678/678 [==============================] - 0s 107us/step - loss: 0.4405 - accuracy: 0.7950 - val_loss: 0.6740 - val_accuracy: 0.6964\n",
      "Epoch 26/100\n",
      "678/678 [==============================] - 0s 122us/step - loss: 0.4365 - accuracy: 0.8009 - val_loss: 0.6719 - val_accuracy: 0.6786\n",
      "Epoch 27/100\n",
      "678/678 [==============================] - 0s 108us/step - loss: 0.4350 - accuracy: 0.8083 - val_loss: 0.6657 - val_accuracy: 0.6845\n",
      "Epoch 28/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.4247 - accuracy: 0.8127 - val_loss: 0.6695 - val_accuracy: 0.6905\n",
      "Epoch 29/100\n",
      "678/678 [==============================] - 0s 108us/step - loss: 0.4218 - accuracy: 0.8201 - val_loss: 0.6649 - val_accuracy: 0.7024\n",
      "Epoch 30/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.4155 - accuracy: 0.8097 - val_loss: 0.6621 - val_accuracy: 0.6786\n",
      "Epoch 31/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.4119 - accuracy: 0.8112 - val_loss: 0.6655 - val_accuracy: 0.6905\n",
      "Epoch 32/100\n",
      "678/678 [==============================] - 0s 108us/step - loss: 0.4088 - accuracy: 0.8097 - val_loss: 0.6621 - val_accuracy: 0.6905\n",
      "Epoch 33/100\n",
      "678/678 [==============================] - 0s 127us/step - loss: 0.4057 - accuracy: 0.8215 - val_loss: 0.6719 - val_accuracy: 0.7083\n",
      "Epoch 34/100\n",
      "678/678 [==============================] - 0s 95us/step - loss: 0.4010 - accuracy: 0.8319 - val_loss: 0.6618 - val_accuracy: 0.6964\n",
      "Epoch 35/100\n",
      "678/678 [==============================] - 0s 92us/step - loss: 0.3972 - accuracy: 0.8260 - val_loss: 0.6530 - val_accuracy: 0.7024\n",
      "Epoch 36/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.3948 - accuracy: 0.8245 - val_loss: 0.6628 - val_accuracy: 0.7083\n",
      "Epoch 37/100\n",
      "678/678 [==============================] - 0s 109us/step - loss: 0.3920 - accuracy: 0.8230 - val_loss: 0.6527 - val_accuracy: 0.7024\n",
      "Epoch 38/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.3887 - accuracy: 0.8245 - val_loss: 0.6576 - val_accuracy: 0.7024\n",
      "Epoch 39/100\n",
      "678/678 [==============================] - 0s 128us/step - loss: 0.3856 - accuracy: 0.8186 - val_loss: 0.6475 - val_accuracy: 0.7083\n",
      "Epoch 40/100\n",
      "678/678 [==============================] - 0s 158us/step - loss: 0.3802 - accuracy: 0.8363 - val_loss: 0.6619 - val_accuracy: 0.6845\n",
      "Epoch 41/100\n",
      "678/678 [==============================] - 0s 167us/step - loss: 0.3784 - accuracy: 0.8260 - val_loss: 0.6570 - val_accuracy: 0.6964\n",
      "Epoch 42/100\n",
      "678/678 [==============================] - 0s 151us/step - loss: 0.3762 - accuracy: 0.8215 - val_loss: 0.6561 - val_accuracy: 0.7083\n",
      "Epoch 43/100\n",
      "678/678 [==============================] - 0s 152us/step - loss: 0.3702 - accuracy: 0.8378 - val_loss: 0.6536 - val_accuracy: 0.7024\n",
      "Epoch 44/100\n",
      "678/678 [==============================] - 0s 137us/step - loss: 0.3701 - accuracy: 0.8319 - val_loss: 0.6456 - val_accuracy: 0.7083\n",
      "Epoch 45/100\n",
      "678/678 [==============================] - 0s 114us/step - loss: 0.3667 - accuracy: 0.8245 - val_loss: 0.6537 - val_accuracy: 0.7024\n",
      "Epoch 46/100\n",
      "678/678 [==============================] - 0s 97us/step - loss: 0.3651 - accuracy: 0.8304 - val_loss: 0.6512 - val_accuracy: 0.6964\n",
      "Epoch 47/100\n",
      "678/678 [==============================] - 0s 94us/step - loss: 0.3623 - accuracy: 0.8304 - val_loss: 0.6563 - val_accuracy: 0.7083\n",
      "Epoch 48/100\n",
      "678/678 [==============================] - 0s 95us/step - loss: 0.3613 - accuracy: 0.8363 - val_loss: 0.6532 - val_accuracy: 0.7143\n",
      "Epoch 49/100\n",
      "678/678 [==============================] - 0s 104us/step - loss: 0.3571 - accuracy: 0.8333 - val_loss: 0.6573 - val_accuracy: 0.7143\n",
      "Epoch 50/100\n",
      "678/678 [==============================] - 0s 107us/step - loss: 0.3564 - accuracy: 0.8378 - val_loss: 0.6481 - val_accuracy: 0.7143\n",
      "Epoch 51/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.3521 - accuracy: 0.8348 - val_loss: 0.6556 - val_accuracy: 0.7202\n",
      "Epoch 52/100\n",
      "678/678 [==============================] - 0s 97us/step - loss: 0.3498 - accuracy: 0.8407 - val_loss: 0.6477 - val_accuracy: 0.7083\n",
      "Epoch 53/100\n",
      "678/678 [==============================] - 0s 108us/step - loss: 0.3491 - accuracy: 0.8407 - val_loss: 0.6427 - val_accuracy: 0.7202\n",
      "Epoch 54/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.3480 - accuracy: 0.8466 - val_loss: 0.6448 - val_accuracy: 0.7143\n",
      "Epoch 55/100\n",
      "678/678 [==============================] - 0s 106us/step - loss: 0.3448 - accuracy: 0.8422 - val_loss: 0.6486 - val_accuracy: 0.7262\n",
      "Epoch 56/100\n",
      "678/678 [==============================] - 0s 104us/step - loss: 0.3411 - accuracy: 0.8437 - val_loss: 0.6489 - val_accuracy: 0.7202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "678/678 [==============================] - 0s 95us/step - loss: 0.3425 - accuracy: 0.8407 - val_loss: 0.6465 - val_accuracy: 0.7262\n",
      "Epoch 58/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.3391 - accuracy: 0.8555 - val_loss: 0.6488 - val_accuracy: 0.7321\n",
      "Epoch 59/100\n",
      "678/678 [==============================] - 0s 112us/step - loss: 0.3359 - accuracy: 0.8525 - val_loss: 0.6512 - val_accuracy: 0.7083\n",
      "Epoch 60/100\n",
      "678/678 [==============================] - 0s 91us/step - loss: 0.3331 - accuracy: 0.8451 - val_loss: 0.6455 - val_accuracy: 0.7143\n",
      "Epoch 61/100\n",
      "678/678 [==============================] - 0s 90us/step - loss: 0.3341 - accuracy: 0.8496 - val_loss: 0.6393 - val_accuracy: 0.7143\n",
      "Epoch 62/100\n",
      "678/678 [==============================] - 0s 94us/step - loss: 0.3302 - accuracy: 0.8525 - val_loss: 0.6474 - val_accuracy: 0.7262\n",
      "Epoch 63/100\n",
      "678/678 [==============================] - 0s 94us/step - loss: 0.3279 - accuracy: 0.8540 - val_loss: 0.6472 - val_accuracy: 0.7262\n",
      "Epoch 64/100\n",
      "678/678 [==============================] - 0s 91us/step - loss: 0.3250 - accuracy: 0.8569 - val_loss: 0.6509 - val_accuracy: 0.7083\n",
      "Epoch 65/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.3241 - accuracy: 0.8555 - val_loss: 0.6490 - val_accuracy: 0.7202\n",
      "Epoch 66/100\n",
      "678/678 [==============================] - 0s 99us/step - loss: 0.3216 - accuracy: 0.8555 - val_loss: 0.6482 - val_accuracy: 0.7262\n",
      "Epoch 67/100\n",
      "678/678 [==============================] - 0s 97us/step - loss: 0.3210 - accuracy: 0.8614 - val_loss: 0.6500 - val_accuracy: 0.7202\n",
      "Epoch 68/100\n",
      "678/678 [==============================] - 0s 97us/step - loss: 0.3186 - accuracy: 0.8658 - val_loss: 0.6446 - val_accuracy: 0.7262\n",
      "Epoch 69/100\n",
      "678/678 [==============================] - 0s 97us/step - loss: 0.3197 - accuracy: 0.8451 - val_loss: 0.6460 - val_accuracy: 0.7262\n",
      "Epoch 70/100\n",
      "678/678 [==============================] - 0s 93us/step - loss: 0.3177 - accuracy: 0.8614 - val_loss: 0.6559 - val_accuracy: 0.7202\n",
      "Epoch 71/100\n",
      "678/678 [==============================] - 0s 109us/step - loss: 0.3152 - accuracy: 0.8628 - val_loss: 0.6586 - val_accuracy: 0.7262\n",
      "Epoch 72/100\n",
      "678/678 [==============================] - 0s 149us/step - loss: 0.3135 - accuracy: 0.8658 - val_loss: 0.6481 - val_accuracy: 0.7143\n",
      "Epoch 73/100\n",
      "678/678 [==============================] - 0s 150us/step - loss: 0.3143 - accuracy: 0.8496 - val_loss: 0.6521 - val_accuracy: 0.7262\n",
      "Epoch 74/100\n",
      "678/678 [==============================] - 0s 155us/step - loss: 0.3098 - accuracy: 0.8687 - val_loss: 0.6582 - val_accuracy: 0.7321\n",
      "Epoch 75/100\n",
      "678/678 [==============================] - 0s 130us/step - loss: 0.3095 - accuracy: 0.8584 - val_loss: 0.6472 - val_accuracy: 0.7262\n",
      "Epoch 76/100\n",
      "678/678 [==============================] - 0s 156us/step - loss: 0.3109 - accuracy: 0.8599 - val_loss: 0.6586 - val_accuracy: 0.7143\n",
      "Epoch 77/100\n",
      "678/678 [==============================] - 0s 112us/step - loss: 0.3061 - accuracy: 0.8687 - val_loss: 0.6491 - val_accuracy: 0.7440\n",
      "Epoch 78/100\n",
      "678/678 [==============================] - 0s 119us/step - loss: 0.3059 - accuracy: 0.8687 - val_loss: 0.6526 - val_accuracy: 0.7262\n",
      "Epoch 79/100\n",
      "678/678 [==============================] - 0s 159us/step - loss: 0.3029 - accuracy: 0.8746 - val_loss: 0.6513 - val_accuracy: 0.7083\n",
      "Epoch 80/100\n",
      "678/678 [==============================] - 0s 165us/step - loss: 0.3026 - accuracy: 0.8643 - val_loss: 0.6626 - val_accuracy: 0.7262\n",
      "Epoch 81/100\n",
      "678/678 [==============================] - 0s 158us/step - loss: 0.2996 - accuracy: 0.8732 - val_loss: 0.6505 - val_accuracy: 0.7202\n",
      "Epoch 82/100\n",
      "678/678 [==============================] - 0s 151us/step - loss: 0.2986 - accuracy: 0.8702 - val_loss: 0.6535 - val_accuracy: 0.7202\n",
      "Epoch 83/100\n",
      "678/678 [==============================] - 0s 153us/step - loss: 0.3002 - accuracy: 0.8687 - val_loss: 0.6598 - val_accuracy: 0.7262\n",
      "Epoch 84/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.2973 - accuracy: 0.8673 - val_loss: 0.6540 - val_accuracy: 0.7202\n",
      "Epoch 85/100\n",
      "678/678 [==============================] - 0s 94us/step - loss: 0.2965 - accuracy: 0.8746 - val_loss: 0.6605 - val_accuracy: 0.7321\n",
      "Epoch 86/100\n",
      "678/678 [==============================] - 0s 92us/step - loss: 0.2955 - accuracy: 0.8761 - val_loss: 0.6616 - val_accuracy: 0.7262\n",
      "Epoch 87/100\n",
      "678/678 [==============================] - 0s 110us/step - loss: 0.2927 - accuracy: 0.8732 - val_loss: 0.6567 - val_accuracy: 0.7262\n",
      "Epoch 88/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.2933 - accuracy: 0.8761 - val_loss: 0.6544 - val_accuracy: 0.7381\n",
      "Epoch 89/100\n",
      "678/678 [==============================] - 0s 106us/step - loss: 0.2888 - accuracy: 0.8673 - val_loss: 0.6606 - val_accuracy: 0.7321\n",
      "Epoch 90/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.2909 - accuracy: 0.8673 - val_loss: 0.6562 - val_accuracy: 0.7202\n",
      "Epoch 91/100\n",
      "678/678 [==============================] - 0s 110us/step - loss: 0.2896 - accuracy: 0.8776 - val_loss: 0.6542 - val_accuracy: 0.7440\n",
      "Epoch 92/100\n",
      "678/678 [==============================] - 0s 111us/step - loss: 0.2865 - accuracy: 0.8805 - val_loss: 0.6546 - val_accuracy: 0.7321\n",
      "Epoch 93/100\n",
      "678/678 [==============================] - 0s 129us/step - loss: 0.2831 - accuracy: 0.8835 - val_loss: 0.6628 - val_accuracy: 0.7262\n",
      "Epoch 94/100\n",
      "678/678 [==============================] - 0s 143us/step - loss: 0.2830 - accuracy: 0.8805 - val_loss: 0.6666 - val_accuracy: 0.7321\n",
      "Epoch 95/100\n",
      "678/678 [==============================] - 0s 162us/step - loss: 0.2809 - accuracy: 0.8791 - val_loss: 0.6607 - val_accuracy: 0.7381\n",
      "Epoch 96/100\n",
      "678/678 [==============================] - 0s 149us/step - loss: 0.2821 - accuracy: 0.8864 - val_loss: 0.6599 - val_accuracy: 0.7381\n",
      "Epoch 97/100\n",
      "678/678 [==============================] - 0s 135us/step - loss: 0.2827 - accuracy: 0.8717 - val_loss: 0.6649 - val_accuracy: 0.7321\n",
      "Epoch 98/100\n",
      "678/678 [==============================] - 0s 156us/step - loss: 0.2819 - accuracy: 0.8820 - val_loss: 0.6679 - val_accuracy: 0.7262\n",
      "Epoch 99/100\n",
      "678/678 [==============================] - 0s 160us/step - loss: 0.2784 - accuracy: 0.8850 - val_loss: 0.6661 - val_accuracy: 0.7202\n",
      "Epoch 100/100\n",
      "678/678 [==============================] - 0s 151us/step - loss: 0.2804 - accuracy: 0.8850 - val_loss: 0.6705 - val_accuracy: 0.7262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a43b0d1d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Model1\n",
    "np.random.seed(67890)\n",
    "model2.fit(PC_X_train, Y_01_train, epochs = 100, validation_data = (PC_X_test, Y_01_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96        39\n",
      "           1       0.60      0.53      0.56        47\n",
      "           2       0.54      0.49      0.51        43\n",
      "           3       0.88      0.92      0.90        39\n",
      "\n",
      "   micro avg       0.74      0.71      0.73       168\n",
      "   macro avg       0.74      0.73      0.73       168\n",
      "weighted avg       0.73      0.71      0.72       168\n",
      " samples avg       0.71      0.71      0.71       168\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/useruser/anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# prediction of classes\n",
    "Y_01_test_pred1 = model2.predict(PC_X_test)\n",
    "# classification report\n",
    "print(classification_report(y_true=Y_01_test, y_pred=Y_01_test_pred1.round(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give your comments here**\n",
    "\n",
    "The new accuracy is 72.62%, which is lower than the accuracy without the PCA, and the classification report shows worse results, compared to the previous one. Recalls for all classes, except 0, became lower, and precision dropped for all classes, except for the 3rd one.\n",
    "\n",
    "We don't think that PCA is very reasonable to conduct in this case, because all the metrics are lower than without it.\n",
    "\n",
    "We tried using only 4 principal components, because they explain more than 80% of variance, which is considered a floor in PCA, and the results were even worse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "307px",
    "left": "909px",
    "right": "20px",
    "top": "120px",
    "width": "338px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
